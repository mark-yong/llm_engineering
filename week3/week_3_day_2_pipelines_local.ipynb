{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRuFso38jxFd"
      },
      "source": [
        "# Welcome to Pipelines!\n",
        "\n",
        "The HuggingFace transformers library provides APIs at two different levels.\n",
        "\n",
        "The High Level API for using open-source models for typical inference tasks is called \"pipelines\". It's incredibly easy to use.\n",
        "\n",
        "You create a pipeline using something like:\n",
        "\n",
        "`my_pipeline = pipeline(\"the_task_I_want_to_do\")`\n",
        "\n",
        "Followed by\n",
        "\n",
        "`result = my_pipeline(my_input)`\n",
        "\n",
        "And that's it!\n",
        "\n",
        "See end of this notebook for a list of all pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tORRYLozax7"
      },
      "source": [
        "## Before we start: 2 important pro-tips for local use:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "Data Science code often gives warnings and messages. They can mostly be safely ignored! Glance over them, and if something goes wrong later, perhaps they can give you a clue.\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "For local use, make sure you have:\n",
        "- Python 3.8+ installed\n",
        "- Required packages installed (see installation cell below)\n",
        "- A Hugging Face account and API token set as an environment variable `HF_TOKEN`\n",
        "- If you have an NVIDIA GPU, make sure CUDA is properly installed. The notebook will automatically detect and use it.\n",
        "- On Mac with Apple Silicon, the notebook will use MPS (Metal Performance Shaders) if available\n",
        "- If no GPU is available, the notebook will fall back to CPU (slower but will work)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyvdfSSv4Cwo"
      },
      "source": [
        "## A sidenote:\n",
        "\n",
        "You may already know this, but just in case you're not familiar with the word \"inference\" that I use here:\n",
        "\n",
        "When working with Data Science models, you could be carrying out 2 very different activities: **training** and **inference**.\n",
        "\n",
        "### 1. Training  \n",
        "\n",
        "**Training** is when you provide a model with data for it to adapt to get better at a task in the future. It does this by updating its internal settings - the parameters or weights of the model. If you're Training a model that's already had some training, the activity is called \"fine-tuning\".\n",
        "\n",
        "### 2. Inference\n",
        "\n",
        "**Inference** is when you are working with a model that has _already been trained_. You are using that model to produce new outputs on new inputs, taking advantage of everything it learned while it was being trained. Inference is also sometimes referred to as \"Execution\" or \"Running a model\".\n",
        "\n",
        "All of our use of APIs for GPT, Claude and Gemini in the last weeks are examples of **inference**. The \"P\" in GPT stands for \"Pre-trained\", meaning that it has already been trained with data (lots of it!) In week 6 we will try fine-tuning GPT ourselves.\n",
        "  \n",
        "The pipelines API in HuggingFace is only for use for **inference** - running a model that has already been trained. In week 7 we will be training our own model, and we will need to use the more advanced HuggingFace APIs that we look at in the up-coming lecture.\n",
        "\n",
        "I recorded this playlist on YouTube with more on parameters, training and inference:  \n",
        "https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ03dQDl2h0D"
      },
      "outputs": [],
      "source": [
        "# Pip installs should come at the top line.\n",
        "# If your Kernel ever resets, you need to run this again.\n",
        "\n",
        "# !pip install -q --upgrade datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bj_PlzfSd5d"
      },
      "outputs": [],
      "source": [
        "# Let's check the GPU and device availability\n",
        "\n",
        "import torch\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Check for CUDA (NVIDIA GPU)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available! Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            print(\"\\nNVIDIA-SMI output:\")\n",
        "            print(result.stdout)\n",
        "    except:\n",
        "        pass\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    print(\"MPS (Apple Silicon) is available!\")\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    print(\"No GPU detected. Will use CPU (this will be slower).\")\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTm7gpG7qhB7"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90GeDKCG6c1v"
      },
      "source": [
        "# Important Note - Hugging Face account\n",
        "\n",
        "In Day 1, we set up a FREE account on https://huggingface.co\n",
        "\n",
        "### If you skipped this:\n",
        "\n",
        "Please go back and do it! Then go to the Avatar menu, Tokens, and create an API token. And make sure it has WRITE permissions!\n",
        "\n",
        "### Setting up your token for local use:\n",
        "\n",
        "Set your Hugging Face token as an environment variable:\n",
        "\n",
        "**Linux/Mac:**\n",
        "```bash\n",
        "export HF_TOKEN=\"your_token_here\"\n",
        "```\n",
        "\n",
        "**Windows (PowerShell):**\n",
        "```powershell\n",
        "$env:HF_TOKEN=\"your_token_here\"\n",
        "```\n",
        "\n",
        "**Windows (Command Prompt):**\n",
        "```cmd\n",
        "set HF_TOKEN=your_token_here\n",
        "```\n",
        "\n",
        "Or you can create a `.env` file in your project directory with:\n",
        "```\n",
        "HF_TOKEN=your_token_here\n",
        "```\n",
        "\n",
        "And load it using python-dotenv (install with `pip install python-dotenv`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7GoH-tT6-xD"
      },
      "outputs": [],
      "source": [
        "# Get Hugging Face token from environment variable\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "if hf_token and hf_token.startswith(\"hf_\"):\n",
        "    print(\"HF token looks good!\")\n",
        "    login(hf_token, add_to_git_credential=True)\n",
        "else:\n",
        "    print(\"HF_TOKEN environment variable is not set or invalid.\")\n",
        "    print(\"Please set it using: export HF_TOKEN='your_token_here'\")\n",
        "    print(\"Or you can enter it manually below (not recommended for security):\")\n",
        "    # Uncomment the lines below if you want to enter the token manually:\n",
        "    # hf_token = input(\"Enter your Hugging Face token: \")\n",
        "    # if hf_token:\n",
        "    #     login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIdFxC_CUnRY"
      },
      "source": [
        "## Using Pipelines from Hugging Face\n",
        "\n",
        "A simple way to run inference for common tasks, without worrying about all the plumbing, picking reasonable defaults.\n",
        "\n",
        "\n",
        "### How it works:\n",
        "\n",
        "STEP 1: Create a pipeline - a function you can then call\n",
        "\n",
        "```python\n",
        "my_pipeline = pipeline(task, model=xx, device=xx)\n",
        "```\n",
        "\n",
        "If you don't specify a model, then Hugging Face picks one for you that's the default for the task. \n",
        "\n",
        "For the device:\n",
        "- Specify `\"cuda\"` for an NVIDIA GPU\n",
        "- Specify `\"mps\"` on a Mac with Apple Silicon\n",
        "- Specify `\"cpu\"` or omit the device parameter to use CPU (slower but works everywhere)\n",
        "\n",
        "The notebook automatically detects the best available device and uses it.\n",
        "\n",
        "\n",
        "STEP 2: Then call it as many times as you want:\n",
        "\n",
        "```python\n",
        "my_pipeline(input1)\n",
        "my_pipeline(input2)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzurQ1d12mBU"
      },
      "outputs": [],
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "my_simple_sentiment_analyzer = pipeline(\"sentiment-analysis\", device=device)\n",
        "result = my_simple_sentiment_analyzer(\"I'm super excited to be on the way to LLM mastery!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6PzFEHzWPSo"
      },
      "outputs": [],
      "source": [
        "result = my_simple_sentiment_analyzer(\"I should be more excited to be on the way to LLM mastery!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-OAalHVT8ED"
      },
      "outputs": [],
      "source": [
        "\n",
        "better_sentiment = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", device=device)\n",
        "result = better_sentiment(\"I should be more excited to be on the way to LLM mastery!!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeSJeFAh21Ra"
      },
      "outputs": [],
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "ner = pipeline(\"ner\", device=device)\n",
        "result = ner(\"AI Engineers are learning about the amazing pipelines from HuggingFace locally from Ed Donner\")\n",
        "for entity in result:\n",
        "  print(entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1fnF2yJ3o6O"
      },
      "outputs": [],
      "source": [
        "# Question Answering with Context\n",
        "\n",
        "question=\"What are Hugging Face pipelines?\"\n",
        "context=\"Pipelines are a high level API for inference of LLMs with common tasks\"\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", device=device)\n",
        "result = question_answerer(question=question, context=context)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjiiWRj231ME"
      },
      "outputs": [],
      "source": [
        "# Text Summarization\n",
        "\n",
        "summarizer = pipeline(\"summarization\", device=device)\n",
        "text = \"\"\"\n",
        "The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
        "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
        "It's an extremely popular library that's widely used by the open-source data science community.\n",
        "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7UMfw324AdO"
      },
      "outputs": [],
      "source": [
        "# Translation\n",
        "\n",
        "translator = pipeline(\"translation_en_to_fr\", device=device)\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGU7ANVaRIkR"
      },
      "outputs": [],
      "source": [
        "# Another translation, showing a model being specified\n",
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSZR309b4IP8"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", device=device)\n",
        "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_vynLSH4YQ7"
      },
      "outputs": [],
      "source": [
        "# Text Generation\n",
        "\n",
        "generator = pipeline(\"text-generation\", device=device)\n",
        "result = generator(\"If there's one thing I want you to remember about using HuggingFace pipelines, it's\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\")\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n",
        "\n",
        "image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgG4kcT_4lO_"
      },
      "outputs": [],
      "source": [
        "# Image Generation - remember this?! Now you know what's going on\n",
        "# Pipelines can be used for diffusion models as well as transformers\n",
        "\n",
        "from IPython.display import display\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "# import torch\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\")\n",
        "# pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo-tensorrt\")\n",
        "pipe.to(\"cuda\")\n",
        "prompt = \"A class of students learning AI engineering in a vibrant pop-art style\"\n",
        "image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCPBE0i4pAAO"
      },
      "outputs": [],
      "source": [
        "# Audio Generation\n",
        "\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from IPython.display import Audio\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device=device)\n",
        "embeddings_dataset = load_dataset(\"matthijs/cmu-arctic-xvectors\", split=\"validation\", trust_remote_code=True)\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "speech = synthesiser(\"Hi to an artificial intelligence engineer, on the way to mastery!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "Audio(speech[\"audio\"], rate=speech[\"sampling_rate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdMBtNNp3FwC"
      },
      "source": [
        "# All the available pipelines\n",
        "\n",
        "Here are all the pipelines available from Transformers and Diffusers.\n",
        "\n",
        "With thanks to student Lucky P for suggesting I include this!\n",
        "\n",
        "There's a list pipelines under the Tasks on this page (you have to scroll down a bit, then expand the parameters to see the Tasks):\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "\n",
        "There's also this list of Tasks for Diffusion models instead of Transformers, following the image generation example where I use DiffusionPipeline above.\n",
        "\n",
        "https://huggingface.co/docs/diffusers/en/api/pipelines/overview\n",
        "\n",
        "If you come up with some cool examples of other pipelines, please share them with me! It's wonderful how HuggingFace makes this advanced AI functionality available for inference with such a simple API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLjkPyBs06KU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
